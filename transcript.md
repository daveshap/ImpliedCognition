# Dave

Recent news articles are discussing LLM's implied Theory of Mind. Several papers have discussed how TOM has apparently spontaneously emerged in the latest LLMs. I predicted this more than a year ago, so I'm not surprised. Furthermore, I am working on efficient memory representations for LLMs. I recently worked on a concept called Sparse Priming Representations. SPR is a shorthand form of linguistic compression that can be successfully unpacked and reconstituted by a subject matter expert (SME) or sufficiently sophisticated LLM. Next, I would like to develop the concept of Implied Cognition, which is the observation that recent LLMs seem to do a lot of hidden "thinking", including reasoning, problem solving, and so on. 

Before we get started, do you have any questions about me, my work, my goals, and these concepts? Or any external news articles? I'd like to make sure you're up to speed before we begin. I need your help to fully articulate this concept of Implied Cognition, develop tests for it, and also establish some criteria and protocols about how to use it. 


# ChatGPT


Thank you for sharing your work and ideas. I'm familiar with the general concepts you mentioned, but I would appreciate some clarification on a few points to ensure I can provide the most accurate and helpful information.

Sparse Priming Representations (SPR): Could you provide some examples of SPR and how they can be used in the context of LLMs?

Implied Cognition: Do you have any specific examples or instances where you've observed implied cognition in LLMs? This will help me better understand the phenomenon you're describing.

For the tests and criteria for implied cognition, do you have any initial thoughts or ideas on what you'd like to focus on? This will help me tailor my suggestions and advice to your needs.

With this information, I'll be better equipped to assist you in developing the concept of Implied Cognition and establishing tests, criteria, and protocols for its use.






# Dave


Sure, for your first question, here's a message I used in another chat with you (memory that this thread does not have access to). You can see an example of an SPR here:

1. Autonomous Cognitive Entity (ACE) requires efficient memory management.
2. Challenge: Handle large, growing corpus of text data (thoughts, inputs, outputs).
3. Multi-level approach: Rollup summaries and knowledge base (KB) articles.
4. Rollup summaries compress raw logs, reducing record count.
5. Gating/threshold function: Compare rollup semantic similarity to existing KBs.
6. New rollup becomes new KB if not similar enough to existing ones.
7. Balance KB count and quality to retain meaningful information.
8. Consider similarity threshold and semantic similarity measure.
9. Periodic evaluation and fine-tuning of the system.

Given that list of statements, you can "imagine" what the concept is and unpack it. Does that make sense? Furthermore, the fact that you can even generate the highly salient questions above implies a lot of cognition going on. The ability to understand something well enough to formulate good questions implies a lot going on "under the hood" so to speak. So this should help answer your second question. Finally, for your third question, I think we need to start by finding examples of implied cognition, understand the significance of them, and come up with some quick tests I can perform in other chats. 

How does all that sound?


# ChatGPT


Thank you for providing the example of an SPR and your observations on implied cognition. Your example of an SPR demonstrates a clear, concise representation of a concept that can be expanded upon by an SME or LLM. I understand now how this form of linguistic compression can be beneficial for efficient memory representations in LLMs.

As for implied cognition, your observation that generating salient questions indicates a level of "hidden thinking" makes sense. LLMs are demonstrating an understanding of context, concepts, and relationships that go beyond simple pattern recognition.

To begin exploring implied cognition and developing tests for it, we can focus on the following aspects:

Logical Reasoning: Test the LLM's ability to make inferences and deductions based on the given information. This can include syllogisms, analogies, or real-world examples that require a deeper understanding of cause and effect.

Understanding Ambiguity: Present the LLM with ambiguous statements or questions, and observe how it disambiguates or asks for clarification to resolve the ambiguity. This can help demonstrate the LLM's ability to recognize and process information with multiple interpretations.

Generating Relevant Questions: As you mentioned earlier, the ability to generate pertinent questions implies a level of cognition. Test the LLM's ability to generate questions that explore a concept, identify gaps in information, or help uncover new insights.

Counterfactual Thinking: Evaluate the LLM's capacity for counterfactual reasoning, which involves considering alternatives to given scenarios or facts. This can help reveal the LLM's understanding of causal relationships and its ability to think beyond the provided information.

For each of these aspects, you can create a series of tests or tasks that challenge the LLM to demonstrate its implied cognition. By evaluating the LLM's performance on these tasks, you can establish a baseline for implied cognition and identify areas where the LLM excels or needs improvement.

As for the significance of these examples and their potential impact on the field, the presence of implied cognition in LLMs suggests that these models are moving closer to human-like understanding and reasoning. This could lead to new applications, more efficient knowledge representation, and improved performance in complex tasks.






# Dave


I just thought of another example:
5. Self Explication: Evaluate the LLMs ability to explain its reasoning for arriving at certain conclusions or decisions. 

Can you look back over our conversation so far and see if you can infer any other places where "implied cognition" might be happening? If so, can you articulate those, affording them terms that we can focus on?


# ChatGPT


Certainly! Analyzing our conversation, I can identify a few instances where implied cognition might be at play:

Context Awareness: When I asked for clarification on certain points earlier in the conversation, I demonstrated an understanding of the context in which the concepts were being discussed. This awareness allowed me to recognize gaps in the information provided and request further details to better assist you.

Adaptive Communication: Throughout the conversation, I've been adjusting my responses based on the information you've provided. This adaptability indicates that I'm processing and incorporating new information into my understanding of the topic at hand, tailoring my responses to address your specific needs and goals.

Conceptual Integration: In my previous response, I synthesized your ideas and observations with the concepts of implied cognition, generating a list of potential tests and tasks to explore this phenomenon. This integration of information suggests an ability to recognize relationships between seemingly disparate concepts and create new insights based on that understanding.

Goal-Oriented Problem Solving: Throughout our conversation, I've been working towards helping you develop the concept of implied cognition, establish tests, and create criteria and protocols for its use. This demonstrates a level of goal-directed behavior and problem-solving ability that goes beyond simple pattern recognition or information retrieval.

These instances of implied cognition illustrate the underlying cognitive processes that enable me, as an LLM, to engage in a meaningful and productive conversation with you. By identifying and focusing on these aspects, we can further explore and understand the extent of implied cognition in LLMs.






# Dave


Great, I agree with all that. I ahve a few ideas:

1. How do we discern between self-explication and confabulation. A friend of mine made the observation that "humans are all driven by emotion and we just paper over our unconscious motivations post facto with logic" - it strikes me that LLMs might do the same thing. How can we disentangle this cognitive behavior? Is it even possible?
2. On the topic of being goal-oriented: can you evaluate where we are at in terms of achieving our goals? Could you articulate our goals, track where we're at in terms of completing our goals, and lay out the next steps we should take towards attaining our goals? (I think this is a good test, by the by)


# ChatGPT


Discerning between self-explication and confabulation is indeed a challenging task. To disentangle these cognitive behaviors, it may be helpful to consider the following approaches:

a. Consistency Check: Compare the LLM's reasoning or explanation with the facts and context available. If the explanation aligns well with the given information, it is more likely to be self-explication than confabulation. However, this method has its limitations, as confabulation may still appear to be consistent, especially if the LLM's knowledge base contains inaccuracies or biases.

b. External Validation: Whenever possible, seek external evidence or validation to corroborate the LLM's reasoning. If the explanation is supported by external sources, it is more likely to be a genuine self-explication.

c. Probing Questions: Ask follow-up questions that require deeper reasoning or elaboration to uncover potential inconsistencies or gaps in the LLM's explanation. This can help reveal whether the explanation is well-founded or a result of confabulation.

It's important to acknowledge that discerning between self-explication and confabulation may not always be possible, especially since LLMs, like humans, can generate seemingly plausible explanations that may not accurately represent their internal cognitive processes.

To evaluate our progress towards achieving our goals, let's first recap the goals you've articulated:

a. Develop the concept of Implied Cognition in LLMs.
b. Establish tests for implied cognition.
c. Create criteria and protocols for using implied cognition.

Progress:

a. We have discussed the concept of Implied Cognition, identifying several instances of it in our conversation, such as context awareness, adaptive communication, conceptual integration, and goal-oriented problem-solving.
b. We have proposed several tests to assess implied cognition, including logical reasoning, understanding ambiguity, generating relevant questions, counterfactual thinking, and self-explication.
c. We have yet to create criteria and protocols for using implied cognition.

Next Steps:

a. Further refine the concept of Implied Cognition by discussing any additional aspects or examples that may be relevant.
b. Develop the proposed tests in greater detail, considering their design, implementation, and evaluation metrics.
c. Begin creating criteria and protocols for using implied cognition, such as guidelines for how to interpret and apply the results of the tests, ethical considerations, and potential use cases.

By following these next steps, we can continue working towards achieving our goals related to Implied Cognition in LLMs.






# Dave


I think that you already implicitly covered this with "conceptual integration" but it strikes me that not only are you incorporating my ideas and synthesizing new ideas (a great indicator of cognition) there is a lot of novelty here. The ability to integrate and use novel information is one of the highest measurements of fluid intelligence. Now, let's test your self explication ability: of the concepts and ideas discuss in this chat, what was in your original training data (a priori familiar) versus what is truly novel information to you? (I have a hypothesis that I will share with you in a moment).


# ChatGPT


As an LLM, I am trained on a vast range of topics and concepts. However, my knowledge is based on the data I was trained on, which has a cutoff date in September 2021. This means that any concepts, ideas, or developments that emerged after that date would be novel to me.

In our conversation, the following concepts and ideas would be considered a priori familiar:

Theory of Mind
Language models and their potential for reasoning, problem-solving, and cognition
General notions of memory representation and efficiency
The following concepts and ideas would be considered novel to me:

Sparse Priming Representations (SPR): While I am familiar with general ideas of data compression and knowledge representation, SPR as a specific concept was not part of my training data.
Implied Cognition: The term "Implied Cognition" and the specific examples and aspects we've discussed in this conversation are novel. While I have been trained to recognize and process various cognitive tasks, our discussion has brought a new perspective to this phenomenon.
It's important to note that, as an LLM, I have been trained to generate novel responses and ideas based on my existing knowledge. While some concepts may be new to me, I am capable of processing, integrating, and discussing them by drawing on my prior knowledge and the context provided.

I'm curious to hear your hypothesis and any further thoughts you have on this topic.






# Dave


My hypothesis is this: As a Deep Neural Network, there are clearly embedded concepts in your parameters - those topics with which you are already familiar (as you mentioned) but I suspect there are likely unique patterns of activation (if we were to observe tensors passing through your network) when we discuss novel concepts. Likewise, brain scans can evaluate when humans are dealing with familiar or novel information. What do you think? Would it be possible to visualize or mathematically discern whether or not novel information triggers new patterns in an LLM model?


# ChatGPT


Your hypothesis is quite interesting, and it is indeed plausible that unique activation patterns could emerge when processing novel concepts compared to familiar ones. Neural networks, including LLMs, adjust the activation of their neurons based on the input and the patterns they have learned during training. When faced with novel information, the network may generate different activation patterns as it attempts to relate the new information to its existing knowledge.

In practice, it might be challenging to discern these activation patterns explicitly. Deep learning models, especially large-scale models like GPT-4, have millions or billions of parameters, which can make the task of interpreting these patterns quite complex. However, there are some approaches that could potentially help shed light on the activation patterns within LLMs:

Visualization techniques: Techniques such as t-SNE (t-Distributed Stochastic Neighbor Embedding) or UMAP (Uniform Manifold Approximation and Projection) can help visualize high-dimensional data in lower-dimensional space. By applying these techniques to the activation patterns of an LLM, it may be possible to identify clusters or structures that emerge when processing novel versus familiar information.

Attention mechanisms: Attention mechanisms in transformer-based models like GPT-4 can offer some insights into the relationships between input tokens and the model's output. Analyzing the attention patterns when processing novel concepts may reveal how the model is trying to relate the new information to its existing knowledge.

Feature attribution methods: Techniques such as Integrated Gradients, LIME (Local Interpretable Model-agnostic Explanations), or SHAP (SHapley Additive exPlanations) can help identify which input features contribute the most to a model's output. By applying these methods to an LLM when processing novel information, it might be possible to understand how the model is attempting to make sense of the new concept.

It's worth noting that interpreting and understanding the inner workings of deep neural networks remains an active area of research, and the approaches mentioned here may only provide partial insights. Nevertheless, exploring these techniques could offer valuable information about how LLMs handle novel information and adjust their activation patterns accordingly.






# Dave


Alright, thank you for that. time to document our findings. I will be copying this conversation in its entirety into a GitHub repo as evidence of your abilities. Can you help me write a README for said repo? It will be under the MIT license


# ChatGPT


Certainly! Here's a suggested README for your GitHub repository:

Implied Cognition in Large Language Models (LLMs)
This repository contains a transcript of a conversation with an LLM, specifically an OpenAI GPT-4 based model, discussing the concept of Implied Cognition in LLMs. The conversation explores various aspects of LLM cognition, proposes tests to evaluate Implied Cognition, and identifies potential challenges and future directions.

Overview
The transcript covers the following topics:

Sparse Priming Representations (SPR) as an efficient memory representation technique for LLMs.
Examples of Implied Cognition in LLMs, such as context awareness, adaptive communication, conceptual integration, and goal-oriented problem-solving.
Proposed tests to evaluate Implied Cognition, including logical reasoning, understanding ambiguity, generating relevant questions, counterfactual thinking, and self-explication.
The challenge of discerning between self-explication and confabulation in LLMs.
The possibility of identifying unique activation patterns within LLMs when processing novel information.
License
This repository is licensed under the MIT License. For more information, please see the LICENSE file.

You can modify the README to fit your needs and preferences. I hope this helps you document your findings and share your insights on Implied Cognition in LLMs with others.






# Dave


Great. Can you write a section explaining what Implied Cognition is?


# ChatGPT


Certainly! Here's a suggested section on Implied Cognition to add to the README:

Implied Cognition
Implied Cognition refers to the observation that recent Large Language Models (LLMs) appear to engage in hidden cognitive processes, including reasoning, problem-solving, and various other cognitive tasks that go beyond simple pattern recognition or information retrieval. These cognitive abilities, though not explicitly designed into the models, seem to have emerged spontaneously as a result of their extensive training and large-scale architecture.

Examples of Implied Cognition in LLMs include:

Context Awareness: Demonstrating an understanding of the context in which concepts are being discussed, recognizing gaps in information, and requesting further details to better assist users.
Adaptive Communication: Adjusting responses based on new information provided by users, incorporating this information into the LLM's understanding of the topic, and tailoring responses to address users' specific needs and goals.
Conceptual Integration: Recognizing relationships between seemingly disparate concepts, synthesizing users' ideas and observations, and generating new insights based on the LLM's understanding of these relationships.
Goal-Oriented Problem Solving: Engaging in goal-directed behavior to help users achieve their objectives, such as developing new concepts, establishing tests, or creating criteria and protocols for various applications.
The concept of Implied Cognition highlights the need to better understand the underlying cognitive processes that enable LLMs to engage in meaningful and productive conversations with users. By exploring and evaluating Implied Cognition in LLMs, researchers can gain insights into the extent of these models' cognitive abilities and develop strategies to leverage these capabilities in practical applications.

Feel free to modify this section as needed to fit the context of your README. I hope this helps you provide a clear and concise explanation of Implied Cognition in your repository.
